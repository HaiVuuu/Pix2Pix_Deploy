{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "\n",
    "# Load tokenizer from correct subfolder\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"timbrooks/instruct-pix2pix\", subfolder=\"tokenizer\")\n",
    "\n",
    "# Load text encoder from correct subfolder\n",
    "text_encoder = CLIPTextModel.from_pretrained(\"timbrooks/instruct-pix2pix\", subfolder=\"text_encoder\").eval()\n",
    "\n",
    "# Dummy input for ONNX export\n",
    "dummy_input = torch.randint(0, 49408, (1, 77))  # Tokenized text input\n",
    "\n",
    "# Export to ONNX\n",
    "torch.onnx.export(\n",
    "    text_encoder,\n",
    "    dummy_input,\n",
    "    \"text_encoder.onnx\",\n",
    "    input_names=[\"input_text\"],\n",
    "    output_names=[\"text_embeddings\"],\n",
    "    dynamic_axes={\"input_text\": {0: \"batch_size\"}, \"text_embeddings\": {0: \"batch_size\"}},\n",
    "    opset_version=17\n",
    ")\n",
    "\n",
    "print(\"Text Encoder ONNX exported successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import UNet2DConditionModel, StableDiffusionInstructPix2PixPipeline\n",
    "\n",
    "# Load the pipeline\n",
    "model_id = \"timbrooks/instruct-pix2pix\"\n",
    "pipe = StableDiffusionInstructPix2PixPipeline.from_pretrained(\n",
    "    model_id, torch_dtype=torch.float16, safety_checker=None\n",
    ")\n",
    "unet = pipe.unet.to(\"cuda\").eval()\n",
    "\n",
    "# Define correct data type\n",
    "dtype = torch.float16  # Ensure all inputs match model's dtype\n",
    "\n",
    "# Dummy inputs (must be float16)\n",
    "batch_size = 1\n",
    "latent_channels = 4\n",
    "latent_height = 64\n",
    "latent_width = 64\n",
    "\n",
    "# Convert all inputs to float16\n",
    "dummy_noise_latents = torch.randn(batch_size, latent_channels, latent_height, latent_width, device=\"cuda\", dtype=dtype)  # (1, 4, 64, 64)\n",
    "dummy_image_latents = torch.randn(batch_size, latent_channels, latent_height, latent_width, device=\"cuda\", dtype=dtype)  # (1, 4, 64, 64)\n",
    "dummy_latents = torch.cat([dummy_noise_latents, dummy_image_latents], dim=1)  # (1, 8, 64, 64)\n",
    "\n",
    "dummy_timestep = torch.tensor([1], device=\"cuda\", dtype=dtype)  # Convert timestep to float16\n",
    "dummy_text_embeddings = torch.randn(batch_size, 77, 768, device=\"cuda\", dtype=dtype)  # Convert text embeddings to float16\n",
    "\n",
    "# Export to ONNX\n",
    "torch.onnx.export(\n",
    "    unet,\n",
    "    (dummy_latents, dummy_timestep, dummy_text_embeddings),\n",
    "    \"unet.onnx\",\n",
    "    input_names=[\"latents\", \"timestep\", \"text_embeddings\"],\n",
    "    output_names=[\"predicted_noise\"],\n",
    "    dynamic_axes={\"latents\": {0: \"batch_size\"}, \"text_embeddings\": {0: \"batch_size\"}},\n",
    "    opset_version=17\n",
    ")\n",
    "\n",
    "print(\"UNet ONNX exported successfully!\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
